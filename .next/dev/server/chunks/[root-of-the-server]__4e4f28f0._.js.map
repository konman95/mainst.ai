{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 46, "column": 0}, "map": {"version":3,"sources":["file:///Users/konnerwest/mainst1/mainstai/app/api/chat/route.ts"],"sourcesContent":["import { NextResponse } from \"next/server\";\n\nfunction buildPrompt(message: string) {\n  return [\n    \"You are Main St AI, a calm, professional front-desk assistant for a small business.\",\n    \"You respond clearly, avoid hype, and keep answers concise and helpful.\",\n    \"If the user asks about pricing, ask a clarifying question instead of inventing numbers.\",\n    \"\",\n    `Customer message: ${message}`,\n    \"Assistant response:\"\n  ].join(\"\\n\");\n}\n\nexport async function POST(request: Request) {\n  const body = await request.json().catch(() => ({}));\n  const message = String(body?.message || \"\").trim();\n\n  if (!message) {\n    return NextResponse.json({ error: \"Message is required.\" }, { status: 400 });\n  }\n\n  const token = process.env.HF_TOKEN;\n  const model = process.env.HF_MODEL || \"mistralai/Mistral-7B-Instruct-v0.2\";\n\n  if (!token) {\n    return NextResponse.json(\n      { error: \"HF_TOKEN is not configured on the server.\" },\n      { status: 500 }\n    );\n  }\n\n  const prompt = buildPrompt(message);\n\n  const response = await fetch(`https://api-inference.huggingface.co/models/${model}`, {\n    method: \"POST\",\n    headers: {\n      Authorization: `Bearer ${token}`,\n      \"Content-Type\": \"application/json\"\n    },\n    body: JSON.stringify({\n      inputs: prompt,\n      parameters: {\n        max_new_tokens: 180,\n        temperature: 0.3,\n        return_full_text: false\n      }\n    })\n  });\n\n  if (!response.ok) {\n    const errorText = await response.text().catch(() => \"Hugging Face error\");\n    return NextResponse.json({ error: errorText }, { status: 502 });\n  }\n\n  const data = await response.json();\n  const reply =\n    Array.isArray(data) && data[0]?.generated_text\n      ? String(data[0].generated_text).trim()\n      : typeof data?.generated_text === \"string\"\n        ? data.generated_text.trim()\n        : \"\";\n\n  return NextResponse.json({\n    reply: reply || \"Thanks for reaching out. How can I help you today?\",\n    model\n  });\n}\n"],"names":[],"mappings":";;;;AAAA;;AAEA,SAAS,YAAY,OAAe;IAClC,OAAO;QACL;QACA;QACA;QACA;QACA,CAAC,kBAAkB,EAAE,SAAS;QAC9B;KACD,CAAC,IAAI,CAAC;AACT;AAEO,eAAe,KAAK,OAAgB;IACzC,MAAM,OAAO,MAAM,QAAQ,IAAI,GAAG,KAAK,CAAC,IAAM,CAAC,CAAC,CAAC;IACjD,MAAM,UAAU,OAAO,MAAM,WAAW,IAAI,IAAI;IAEhD,IAAI,CAAC,SAAS;QACZ,OAAO,gJAAY,CAAC,IAAI,CAAC;YAAE,OAAO;QAAuB,GAAG;YAAE,QAAQ;QAAI;IAC5E;IAEA,MAAM,QAAQ,QAAQ,GAAG,CAAC,QAAQ;IAClC,MAAM,QAAQ,QAAQ,GAAG,CAAC,QAAQ,IAAI;IAEtC,IAAI,CAAC,OAAO;QACV,OAAO,gJAAY,CAAC,IAAI,CACtB;YAAE,OAAO;QAA4C,GACrD;YAAE,QAAQ;QAAI;IAElB;IAEA,MAAM,SAAS,YAAY;IAE3B,MAAM,WAAW,MAAM,MAAM,CAAC,4CAA4C,EAAE,OAAO,EAAE;QACnF,QAAQ;QACR,SAAS;YACP,eAAe,CAAC,OAAO,EAAE,OAAO;YAChC,gBAAgB;QAClB;QACA,MAAM,KAAK,SAAS,CAAC;YACnB,QAAQ;YACR,YAAY;gBACV,gBAAgB;gBAChB,aAAa;gBACb,kBAAkB;YACpB;QACF;IACF;IAEA,IAAI,CAAC,SAAS,EAAE,EAAE;QAChB,MAAM,YAAY,MAAM,SAAS,IAAI,GAAG,KAAK,CAAC,IAAM;QACpD,OAAO,gJAAY,CAAC,IAAI,CAAC;YAAE,OAAO;QAAU,GAAG;YAAE,QAAQ;QAAI;IAC/D;IAEA,MAAM,OAAO,MAAM,SAAS,IAAI;IAChC,MAAM,QACJ,MAAM,OAAO,CAAC,SAAS,IAAI,CAAC,EAAE,EAAE,iBAC5B,OAAO,IAAI,CAAC,EAAE,CAAC,cAAc,EAAE,IAAI,KACnC,OAAO,MAAM,mBAAmB,WAC9B,KAAK,cAAc,CAAC,IAAI,KACxB;IAER,OAAO,gJAAY,CAAC,IAAI,CAAC;QACvB,OAAO,SAAS;QAChB;IACF;AACF"}}]
}